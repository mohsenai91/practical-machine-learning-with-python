{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'soft_unicode' from 'markupsafe' (c:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\markupsafe\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\spacy\\__init__.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcli\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfo\u001b[39;00m \u001b[39mimport\u001b[39;00m info  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mglossary\u001b[39;00m \u001b[39mimport\u001b[39;00m explain  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mabout\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\spacy\\cli\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdebug_config\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_config  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdebug_model\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_model  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdebug_diff\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_diff  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevaluate\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m apply  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\spacy\\cli\\debug_diff.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m debug_cli, Arg, Opt, show_validation_error, parse_config_overrides\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m load_config\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minit_config\u001b[39;00m \u001b[39mimport\u001b[39;00m init_config, Optimizations\n\u001b[0;32m     13\u001b[0m \u001b[39m@debug_cli\u001b[39m\u001b[39m.\u001b[39mcommand(\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdiff-config\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     context_settings\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mallow_extra_args\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mignore_unknown_options\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39m# fmt: on\u001b[39;00m\n\u001b[0;32m     27\u001b[0m ):\n\u001b[0;32m     28\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Show a diff of a config file with respect to spaCy's defaults or another config file. If\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    additional settings were used in the creation of the config file, then you\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    must supply these as extra parameters to the command when comparing to the default settings. The generated diff\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/cli#debug-diff\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\spacy\\cli\\init_config.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msrsly\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjinja2\u001b[39;00m \u001b[39mimport\u001b[39;00m Template\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m util\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage\u001b[39;00m \u001b[39mimport\u001b[39;00m DEFAULT_CONFIG_PRETRAIN_PATH\n",
      "File \u001b[1;32mc:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\jinja2\\__init__.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2.10\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[39m# high level interface\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjinja2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironment\u001b[39;00m \u001b[39mimport\u001b[39;00m Environment, Template\n\u001b[0;32m     35\u001b[0m \u001b[39m# loaders\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjinja2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloaders\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLoader, FileSystemLoader, PackageLoader, \\\n\u001b[0;32m     37\u001b[0m      DictLoader, FunctionLoader, PrefixLoader, ChoiceLoader, \\\n\u001b[0;32m     38\u001b[0m      ModuleLoader\n",
      "File \u001b[1;32mc:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\jinja2\\environment.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mweakref\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m reduce, partial\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjinja2\u001b[39;00m \u001b[39mimport\u001b[39;00m nodes\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjinja2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdefaults\u001b[39;00m \u001b[39mimport\u001b[39;00m BLOCK_START_STRING, \\\n\u001b[0;32m     17\u001b[0m      BLOCK_END_STRING, VARIABLE_START_STRING, VARIABLE_END_STRING, \\\n\u001b[0;32m     18\u001b[0m      COMMENT_START_STRING, COMMENT_END_STRING, LINE_STATEMENT_PREFIX, \\\n\u001b[0;32m     19\u001b[0m      LINE_COMMENT_PREFIX, TRIM_BLOCKS, NEWLINE_SEQUENCE, \\\n\u001b[0;32m     20\u001b[0m      DEFAULT_FILTERS, DEFAULT_TESTS, DEFAULT_NAMESPACE, \\\n\u001b[0;32m     21\u001b[0m      DEFAULT_POLICIES, KEEP_TRAILING_NEWLINE, LSTRIP_BLOCKS\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjinja2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlexer\u001b[39;00m \u001b[39mimport\u001b[39;00m get_lexer, TokenStream\n",
      "File \u001b[1;32mc:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\jinja2\\nodes.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moperator\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m deque\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjinja2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m Markup\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjinja2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_compat\u001b[39;00m \u001b[39mimport\u001b[39;00m izip, with_metaclass, text_type, PY2\n\u001b[0;32m     23\u001b[0m \u001b[39m#: the types we support for context functions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\jinja2\\utils.py:647\u001b[0m\n\u001b[0;32m    643\u001b[0m     have_async_gen \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[39m# Imported here because that's where it was in the past\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarkupsafe\u001b[39;00m \u001b[39mimport\u001b[39;00m Markup, escape, soft_unicode\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'soft_unicode' from 'markupsafe' (c:\\Users\\mh.karimi\\Documents\\GitHub\\practical-machine-learning-with-python\\.venv\\lib\\site-packages\\markupsafe\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text - strip HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize text corpus - tying it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \\n              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\\n              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\\n              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\\n              got a headstart!</p>\\n           \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
    "              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\n",
    "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
    "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
    "              got a headstart!</p>\n",
    "           \"\"\"\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Hello can you hear me I just heard about Python It is an amazing language which can be used for Scripting Web development Information Retrieval Natural Language Processing Machine Learning Artificial Intelligence What are you waiting for Go and get started He is learning she is learning they have already got a headstart ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus([document], text_lemmatization=False, stopword_removal=False, text_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello hello hear hear python amazing language use scripting web development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus([document])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
